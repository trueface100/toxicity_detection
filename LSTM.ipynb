{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "76e91619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6380c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    comment_csv=\"data/preprocessed_train_data.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    # 날짜와 경로 정보\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/model3_LSTM\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    max_sequence_length=1309,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=10,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "80a33352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, comment_df, vectorizer):\n",
    "        self.comment_df = comment_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self._max_seq_length = args.max_sequence_length\n",
    "\n",
    "        self.train_df = self.comment_df[self.comment_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.comment_df[self.comment_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.comment_df[self.comment_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                             'val': (self.val_df, self.validation_size), \n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, comment_csv):  \n",
    "        comment_df = pd.read_csv(comment_csv)\n",
    "        return cls(comment_df, CommentVectorizer.from_dataframe(comment_df))\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, comment_csv, vectorizer_filepath):\n",
    "        comment_df = pd.read_csv(comment_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(comment_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CommentVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        comment_vector, vector_length = self._vectorizer.vectorize(row.comment, self._max_seq_length)\n",
    "        toxicity_label = row.toxicity\n",
    "\n",
    "        return {'x_data': comment_vector, 'y_target': toxicity_label, 'x_length': vector_length}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f3704294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\", mask_token=\"<MASK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._unk_token = unk_token\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "        self._mask_token = mask_token\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx.get(token, self.unk_index)\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6df5fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b708c94",
   "metadata": {},
   "source": [
    "### `Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "35257421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentVectorizer(object):\n",
    "    def __init__(self, token_vocab, label_vocab):\n",
    "        self.token_vocab = token_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "\n",
    "    def vectorize(self, comment, max_seq_length):\n",
    "        indices = [self.token_vocab.lookup_token(token) for token in comment.split(\" \")]\n",
    "        \n",
    "        if len(indices) > max_seq_length:\n",
    "            indices = indices[:max_seq_length] \n",
    "        elif len(indices) < max_seq_length:\n",
    "            indices += [self.token_vocab.mask_index] * (max_seq_length - len(indices)) \n",
    "\n",
    "        return np.array(indices), len(indices)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, comment_df):\n",
    "        token_vocab = Vocabulary(add_unk=True)\n",
    "        label_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for index, row in comment_df.iterrows():\n",
    "            for token in row.comment.split(\" \"):\n",
    "                token_vocab.add_token(token)\n",
    "            label_vocab.add_token(row.toxicity)\n",
    "\n",
    "        return cls(token_vocab, label_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        token_vocab = Vocabulary.from_serializable(contents['token_vocab'])\n",
    "        label_vocab = Vocabulary.from_serializable(contents['label_vocab'])\n",
    "\n",
    "        return cls(token_vocab=token_vocab, label_vocab=label_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_vocab': self.token_vocab.to_serializable(), \n",
    "                'label_vocab': self.label_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c052dd",
   "metadata": {},
   "source": [
    "### `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f3fa38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            if name == 'x_data':\n",
    "                tensor = tensor[:, :args.max_sequence_length]\n",
    "                if tensor.size(1) < args.max_sequence_length:\n",
    "                    tensor = F.pad(tensor, pad=(0, args.max_sequence_length - tensor.size(1)), \n",
    "                                   mode='constant', value=dataset._vectorizer.token_vocab.mask_index)\n",
    "                out_data_dict[name] = tensor.to(device)\n",
    "            else:\n",
    "                out_data_dict[name] = tensor.to(device)\n",
    "        x_lengths = torch.sum(out_data_dict['x_data'] != dataset._vectorizer.token_vocab.mask_index, dim=1)\n",
    "        yield out_data_dict, x_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0f9c1",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e930e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentClassificationModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, rnn_hidden_size, \n",
    "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "\n",
    "        super(CommentClassificationModel, self).__init__()\n",
    "        \n",
    "        self.word_emb = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size,\n",
    "                           hidden_size=rnn_hidden_size,\n",
    "                           batch_first=batch_first)\n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
    "                            out_features=1)\n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, x_lengths, apply_sigmoid=False):\n",
    "        x_embedded = self.word_emb(x_in)\n",
    "        x_packed = rnn_utils.pack_padded_sequence(x_embedded, x_lengths.cpu(), \n",
    "                                                  batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_out, (hidden, cell) = self.rnn(x_packed)\n",
    "        y_out, _ = rnn_utils.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        y_out = y_out[range(len(x_lengths)), x_lengths - 1, :]\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
    "\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "\n",
    "        return y_out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6f58c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "505e3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, 'vectorizer.json')\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,'model.pth')\n",
    "    \n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "36842cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= loss_tm1:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    \n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    predictions = y_pred > 0.5\n",
    "    correct = (predictions == y_true).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy * 100\n",
    "\n",
    "def binary_classification_loss(y_pred, y_true):\n",
    "    return F.binary_cross_entropy_with_logits(y_pred, y_true.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e2449c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# 데이터셋과 Vectorizer\n",
    "dataset = CommentDataset.load_dataset_and_make_vectorizer(args.comment_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# 모델\n",
    "model = CommentClassificationModel(embedding_size=args.embedding_size,\n",
    "                                   vocab_size=len(vectorizer.token_vocab),\n",
    "                                   rnn_hidden_size=args.rnn_hidden_size,\n",
    "                                   padding_idx=vectorizer.token_vocab.mask_index)\n",
    "model = model.to(args.device)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ff7bd4",
   "metadata": {},
   "source": [
    "## 훈련 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "dc6a0fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2cca61314e4dd2a6df01892862bb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fde9ae9e7bb47a0b42968150f55a074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa597abe09844a818c484a492468ecfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                               total=args.num_epochs,\n",
    "                               position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
    "                               total=dataset.get_num_batches(args.batch_size), \n",
    "                               position=1, \n",
    "                               leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
    "                             total=dataset.get_num_batches(args.batch_size), \n",
    "                             position=1, \n",
    "                             leave=True)\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    # 훈련 세트에 대한 순회\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_index, (batch_dict, x_lengths) in enumerate(batch_generator):\n",
    "        # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "        # --------------------------------------\n",
    "        # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 단계 2. 출력을 계산합니다\n",
    "        y_pred = model(x_in=batch_dict['x_data'], x_lengths=x_lengths)\n",
    "\n",
    "        # 단계 3. 손실을 계산합니다\n",
    "        loss = binary_classification_loss(y_pred, batch_dict['y_target'])\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "\n",
    "        # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "        loss.backward()\n",
    "\n",
    "        # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "\n",
    "        # 이동 손실과 이동 정확도를 계산합니다\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # 진행 상태 막대 업데이트\n",
    "        train_bar.set_postfix(loss=running_loss,\n",
    "                              acc=running_acc,\n",
    "                              epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # 검증 세트에 대한 순회\n",
    "\n",
    "    # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    model.eval()\n",
    "\n",
    "    for batch_index, (batch_dict, x_lengths) in enumerate(batch_generator):\n",
    "        # 단계 1. 출력을 계산합니다\n",
    "        y_pred = model(x_in=batch_dict['x_data'], x_lengths=x_lengths)\n",
    "\n",
    "        # 단계 2. 손실을 계산합니다\n",
    "        loss = binary_classification_loss(y_pred, batch_dict['y_target'])\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "\n",
    "        # 단계 3. 이동 손실과 이동 정확도를 계산합니다\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # 진행 상태 막대 업데이트\n",
    "        val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                        epoch=epoch_index)\n",
    "        val_bar.update()\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=model, \n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20ddba",
   "metadata": {},
   "source": [
    "## 테스트 세트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e1ffe778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "model = model.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "model.eval()\n",
    "\n",
    "for batch_index, (batch_dict, x_lengths) in enumerate(batch_generator):\n",
    "    x_data = batch_dict['x_data']\n",
    "    y_target = batch_dict['y_target']\n",
    "    \n",
    "    # 모델 실행\n",
    "    y_pred = model(x_in=x_data, x_lengths=x_lengths)\n",
    "\n",
    "    # 손실 및 정확도 계산\n",
    "    loss = binary_classification_loss(y_pred, y_target)\n",
    "    acc_t = compute_accuracy(y_pred, y_target)\n",
    "\n",
    "    # 이동 평균 손실 및 정확도 업데이트\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "# 최종 손실 및 정확도 저장\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e88b29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.1810012078413399\n",
      "테스트 정확도: 94.2540283203125\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66b43b",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "67fe893f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_df = pd.read_csv('data/test_for_inference_preprocessed.csv')\n",
    "test_data_df['comment'] = test_data_df['comment'].astype(str)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "model = CommentClassificationModel(embedding_size=args.embedding_size, \n",
    "                                   vocab_size=len(vectorizer.token_vocab), \n",
    "                                   rnn_hidden_size=args.rnn_hidden_size)\n",
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "model.eval()\n",
    "model = model.to(args.device)\n",
    "\n",
    "def predict_proba(model, vectorizer, comment, device, max_sequence_length):\n",
    "    comment_vector, vector_length = vectorizer.vectorize(comment, max_sequence_length)\n",
    "    vectorized_comment = torch.tensor(comment_vector, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    length_tensor = torch.tensor([vector_length], dtype=torch.long).to(device)\n",
    "\n",
    "    result = model(vectorized_comment, length_tensor)\n",
    "    probability = torch.sigmoid(result).detach().cpu().item()\n",
    "    return probability\n",
    "\n",
    "results = []\n",
    "for comment in test_data_df['comment']:\n",
    "    probability = predict_proba(model, vectorizer, comment, args.device, args.max_sequence_length)\n",
    "    prediction = 1 if probability > 0.5 else 0\n",
    "    results.append({'probability': probability, 'pred': prediction})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('result/inferenced_by_LSTM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e1c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
